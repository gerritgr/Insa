{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29569a76-c280-4900-9b3f-3f0037e7b877",
   "metadata": {},
   "source": [
    "# ðŸ•¸ï¸ ðŸ”¬ INSA - Interaction Graph Inferecne via Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736244b0-fe5a-4fee-b6ed-08177749ab17",
   "metadata": {},
   "source": [
    "### Todos\n",
    "\n",
    "- Clear and tidy outputs.\n",
    "- Make it runnable without wandb.\n",
    "- Ensure consistent indentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a94a2-5654-4d15-916e-be02a1116bbc",
   "metadata": {},
   "source": [
    "### WandB Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78229b5-73e9-4ca5-af36-00ed8c950207",
   "metadata": {},
   "source": [
    "Currently, you must have a WandB account to execute the code. Please insert your WandB token in the designated area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f07be-c0ae-4bb9-b7cc-d516ae353aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "\n",
    "PROJECT_NAME = \"insa\"\n",
    "WANDB_MODE = \"online\"   \n",
    "USE_WANDB = True\n",
    "NUM_RUNS = 10\n",
    "\n",
    "DIFFERENT_PARAMS_FOR_SNAPSHOTS = False\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"main.ipynb\"\n",
    "\n",
    "def get_wand_api_key():\n",
    "    return \"\"  # ADD TOKEN HERE!\n",
    "\n",
    "\n",
    "wandb.login(key=get_wand_api_key())\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"insa\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"position\",\n",
    "        \"goal\": \"minimize\",\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"num_epoch\": {\"values\": [15]},\n",
    "        \"num_train_sample\": {\"values\": [20000]},\n",
    "        \"num_test_sample\": {\"values\": [10000]},\n",
    "        \"batch_size\": {\"values\": [2048]},\n",
    "        \"dropout\": {\"values\": [0.15]},\n",
    "        \"layer_num\": {\"values\": [5]},\n",
    "        \"hidden_size_scale\":  {\"values\": [2]},\n",
    "        \"use_train_as_test\": {\"values\": [False]},\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_config(name):\n",
    "    try:\n",
    "        assert(USE_WANDB)\n",
    "        return wandb.config[name]\n",
    "    except:\n",
    "        return sweep_config[\"parameters\"][name][\"values\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eeb77c-daeb-41f4-a7e9-c0f1e1f1d1bc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011e64c-ba40-423d-aa4a-28365e0095a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math, random, time, traceback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import netrd\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9acb0-9377-45d5-bd33-42a63509819a",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74130-f703-4f79-8d34-0ebb24b73790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exclude_index(vector, index):\n",
    "    if isinstance(vector, torch.Tensor):\n",
    "        before_index = vector[:, :index]\n",
    "        after_index = vector[:, index+1:]\n",
    "        new_vector = torch.cat((before_index, after_index), dim=1)\n",
    "        return new_vector, vector[:, index:index+1].clone()\n",
    "    elif isinstance(vector, np.ndarray):\n",
    "        before_index = vector[:, :index]\n",
    "        after_index = vector[:, index+1:]\n",
    "        new_vector = np.concatenate((before_index, after_index), axis=1)\n",
    "        return new_vector, np.copy(vector[:, index:index+1])\n",
    "    else:\n",
    "        raise TypeError(\"Input vector must be a PyTorch Tensor or a numpy ndarray\")\n",
    "        \n",
    "        \n",
    "def state_to_color(state):\n",
    "    if len(state) == 1:\n",
    "        cmap = plt.get_cmap(\"viridis\")\n",
    "        return cmap(state[0])\n",
    "    colors = ['blue', 'red', 'black', 'green', 'orange', 'yellow', 'pink']\n",
    "    return colors[np.argmax(state) % 6]\n",
    "\n",
    "        \n",
    "def plot_snapshots(G, snapshots, exp_name, max_plot_num=10):\n",
    "    try:\n",
    "        for i, snapshot in enumerate(snapshots):\n",
    "            assert(len(snapshot) == G.number_of_nodes())\n",
    "            if max_plot_num is not None and i >= max_plot_num:\n",
    "                break\n",
    "            node_color = [state_to_color(snapshot[j]) for j in range(len(snapshot))]\n",
    "            plt.clf()\n",
    "            nx.draw_networkx(G, node_color=node_color, pos=nx.spectral_layout(G))\n",
    "            plt.savefig(f'{exp_name}_snapshot_{i:03}_spec.png')\n",
    "            plt.clf()\n",
    "            nx.draw_networkx(G, node_color=node_color, pos=nx.kamada_kawai_layout(G))\n",
    "            plt.savefig(f'{exp_name}_snapshot_{i:03}_kam.png')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not draw snapshot: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "def get_position(graph_distance_dict):\n",
    "    #print(\"get_position(graph_distance_dict)\", (graph_distance_dict))\n",
    "    graph_distance = [(k,v) for k,v in graph_distance_dict.items() if \"hierachical_down\" in k]\n",
    "    graph_distance = sorted(graph_distance, key=lambda x: x[1])\n",
    "    graph_distance = [k for k,v in graph_distance]\n",
    "    #print(\"graph_distance \",graph_distance)\n",
    "    our_method = \"our method (hierachical_down)\"\n",
    "    pos = None\n",
    "    for i, k in enumerate(graph_distance):\n",
    "        if k == our_method:\n",
    "            return i\n",
    "    raise ValueError(\"method not found\")\n",
    "\n",
    "    \n",
    "def get_node_num_and_dim_from_loader(traintest_loader):\n",
    "    first_data_batch = next(iter(traintest_loader))\n",
    "    node_num = first_data_batch.shape[1]\n",
    "    node_dim = 1\n",
    "    \n",
    "    try:\n",
    "        node_dim = first_data_batch.shape[2]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    return node_num, node_dim\n",
    "\n",
    "\n",
    "def get_node_num_and_dim_from_loader(traintest_loader):\n",
    "    first_data_batch = next(iter(traintest_loader))\n",
    "    node_num = first_data_batch.shape[1]\n",
    "    node_dim = 1\n",
    "    try:\n",
    "        node_dim = first_data_batch.shape[2]\n",
    "    except:\n",
    "        pass\n",
    "    return node_num, node_dim\n",
    "\n",
    "\n",
    "def clean_shuffle_graph(G):\n",
    "    node_mapping = dict(zip(sorted(G.nodes()), sorted(G.nodes(), key=lambda _: random.random()))) \n",
    "    G = nx.relabel_nodes(G, node_mapping)\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    if not nx.is_connected(G):\n",
    "        print('Graph is not connected, try a differnt one.')\n",
    "        assert(nx.is_connected(G))\n",
    "    return G\n",
    "\n",
    "\n",
    "def set_seed(exp_name):\n",
    "    config_name = \"\"\n",
    "    config_keys = sorted(sweep_config[\"parameters\"].keys())\n",
    "    config_name = str([(k,repr(get_config(k))) for k in config_keys])\n",
    "    name_as_int = int(str_to_float(config_name + exp_name)*10000000)\n",
    "    np.random.seed(name_as_int)\n",
    "    torch.random.manual_seed(name_as_int)\n",
    "    random.seed(name_as_int)\n",
    "\n",
    "# from stack overflow\n",
    "def str_to_float(s, encoding=\"utf-8\"):\n",
    "    from zlib import crc32\n",
    "    def bytes_to_float(b):\n",
    "        return float(crc32(b) & 0xffffffff) / 2**32\n",
    "    return bytes_to_float(s.encode(encoding))\n",
    "\n",
    "\n",
    "def str_to_int(s):\n",
    "    return int(str_to_float(s)*10000000)\n",
    "\n",
    "def visualize_matrix(matrix, filepath):\n",
    "    print(filepath)\n",
    "    plt.clf()\n",
    "    plt.imshow(matrix, cmap='gray')\n",
    "    plt.colorbar() # optional, to see color map scale\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def add_exp_to_results(exp_name, results, graph_distance_dict, graph_position_dict, time_elapsed_dict, filename=\"results.csv\"):\n",
    "    method_names = sorted(graph_distance_dict.keys())\n",
    "    for method in method_names:\n",
    "        results[\"exp_name\"].append(exp_name)\n",
    "        results[\"method\"].append(method)\n",
    "        results[\"graph_distance\"].append(graph_distance_dict[method])\n",
    "        results[\"position\"].append(graph_position_dict[method])\n",
    "        try:\n",
    "            results[\"runtime\"].append(time_elapsed_dict[method])\n",
    "        except:\n",
    "            results[\"runtime\"].append(-1.0)\n",
    "\n",
    "    results_pandas = pd.DataFrame(results)\n",
    "    results_pandas.to_csv(filename)\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_graph_distance(adj_matrix_gt, adj_matrix_pred):\n",
    "    if 'numpy' not in str(type(adj_matrix_gt)).lower():\n",
    "        adj_matrix_gt = nx.to_numpy_array(adj_matrix_gt)\n",
    "    if 'numpy' not in str(type(adj_matrix_pred)).lower():\n",
    "        adj_matrix_pred = nx.to_numpy_array(adj_matrix_pred) \n",
    "\n",
    "    z_ij = adj_matrix_gt - adj_matrix_pred\n",
    "    z_ij = np.abs(z_ij)\n",
    "    np.fill_diagonal(z_ij, 0.0) # should not need this\n",
    "    return np.sum(z_ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54367401-3d24-43c4-810f-6b2b077b81f2",
   "metadata": {},
   "source": [
    "## Tresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29718ec7-ba8d-4854-840b-fed59271fdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_elements_gmm(vector):\n",
    "    X = np.array(vector).reshape(-1, 1)\n",
    "    gmm = GaussianMixture(n_components=2)\n",
    "    gmm.fit(X)\n",
    "    labels = gmm.predict(X)\n",
    "    return labels\n",
    "\n",
    "def cluster_elements_hierachical(vector):\n",
    "    X = np.array(vector).reshape(-1, 1)\n",
    "    clustering = AgglomerativeClustering(n_clusters=2)\n",
    "    clustering.fit(X)\n",
    "    labels = clustering.labels_\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def cluster2d_elements_normalize(matrix):\n",
    "    for _ in range(100):\n",
    "        row_sums = matrix.sum(axis=1)\n",
    "        matrix = matrix / row_sums[:, np.newaxis]\n",
    "        column_sums = matrix.sum(axis=0)\n",
    "        matrix = matrix / column_sums[np.newaxis, :]\n",
    "    return binarize_weight_matrix(matrix, cluster_method = \"hierachical\")\n",
    "\n",
    "\n",
    "def cluster2d_elements_multiply(matrix):\n",
    "    binary_adj = np.zeros_like(matrix)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row = np.delete(matrix[i,:], i)\n",
    "        column = np.delete(matrix[:, i], i)\n",
    "        row = row/np.max(row)\n",
    "        column = column/np.max(column)\n",
    "        combined = row * column\n",
    "        labels = cluster_elements_hierachical(combined).flatten()\n",
    "        binary_adj[i, :i] = labels[:i]\n",
    "        binary_adj[i, i+1:] = labels[i:]\n",
    "\n",
    "        binary_adj[:i, i] = labels[:i]\n",
    "        binary_adj[i+1:, i] = labels[i:]\n",
    "\n",
    "    return binary_adj\n",
    "\n",
    "def cluster_elements_maxmargin(vector):\n",
    "    vector = np.array(vector).flatten()\n",
    "    best_threshold = None\n",
    "    best_score = float('-inf')\n",
    "    epsilon = 1e-10  # to avoid boundary values\n",
    "    grid = np.linspace(np.min(vector) + epsilon, np.max(vector) - epsilon, 1000)\n",
    "    \n",
    "    for threshold in grid:\n",
    "        largest_smaller = np.max(vector[vector < threshold])\n",
    "        smallest_larger = np.min(vector[vector > threshold])\n",
    "        score = np.sqrt(threshold - largest_smaller) + np.sqrt(smallest_larger - threshold)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    labels = (vector > best_threshold).astype(int)\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_cluster_methods():\n",
    "    return {\"gmm\": cluster_elements_gmm, \"hierachical\": cluster_elements_hierachical, \"maxmargin\": cluster_elements_maxmargin, \"2dmultiply\": cluster2d_elements_multiply, \"2dnormalize\":cluster2d_elements_normalize}\n",
    "\n",
    "def cluster_elements(vector, cluster_method=None):\n",
    "    if cluster_method is None:\n",
    "        cluster_method = \"hierachical\"\n",
    "    if \"str\" in str(type(cluster_method)):\n",
    "        cluster_method = get_cluster_methods()[cluster_method.lower()]\n",
    "\n",
    "    X = np.array(vector).reshape(-1, 1)\n",
    "    labels = cluster_method(vector)\n",
    "\n",
    "    if np.mean(X[labels == 0]) > np.mean(X[labels == 1]):\n",
    "        labels = 1 - labels\n",
    "    if np.min(labels) == np.max(labels): # only one cluster\n",
    "        labels = labels * 0 + 1 # assume a node connected to all other nodes instaed of an isolated node\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec5b25-1ab9-424b-8ae7-e8024496e45a",
   "metadata": {},
   "source": [
    "## Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8ef05-028c-44da-81dd-e26aa6b7717f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_graph_generators():\n",
    "\n",
    "  G_grid_50 = nx.grid_2d_graph(5,5)\n",
    "  G_erdos_50 = nx.erdos_renyi_graph(50, 0.15, seed=42)\n",
    "  G_wsn_50 = nx.newman_watts_strogatz_graph(50, 4, 0.15, seed=42)\n",
    "\n",
    "  ground_truth_graphset = dict()\n",
    "\n",
    "  for n in [50]:\n",
    "    for name in [\"G_erdos_\", \"G_wsn_\", \"G_grid_\"]:\n",
    "      if \"grid\" in name and n in [25, 200]:\n",
    "        continue\n",
    "      try:\n",
    "        ground_truth_graphset[name+str(n)] = eval(name+str(n))\n",
    "      except:\n",
    "        print(\"coud not create\", name+str(n))\n",
    "\n",
    "  ground_truth_graphset = {graph_name: clean_shuffle_graph(g) for graph_name, g in ground_truth_graphset.items()}\n",
    "  return ground_truth_graphset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5401242-178c-400f-9cc9-994d16ec6c1a",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e798f-3d6a-46f8-b12b-87b6736376b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binarize_weight_matrix(W, cluster_method = None):\n",
    "  adj_matrix = np.zeros_like(W)\n",
    "  if \"2d\" in cluster_method:\n",
    "    cluster_method = get_cluster_methods()[cluster_method.lower()]\n",
    "    return cluster_method(W)\n",
    "  for i in range(W.shape[0]):\n",
    "    row, _ = exclude_index(W[i,:].reshape(1,-1), i)\n",
    "    row_binary = cluster_elements(row, cluster_method=cluster_method).flatten()\n",
    "\n",
    "    adj_matrix[i, :i] = row_binary[:i]\n",
    "    adj_matrix[i, i+1:] = row_binary[i:]\n",
    "\n",
    "  return adj_matrix\n",
    "\n",
    "\n",
    "def binarize_weight_matrix_multi(impact_scores):\n",
    "  # Compute binary impact score (graph)\n",
    "  cluster_method_names = sorted(get_cluster_methods().keys())\n",
    "  impact_scores_binary = dict()\n",
    "  for cluster_method in cluster_method_names:\n",
    "    impact_scores_binary[cluster_method] = binarize_weight_matrix(impact_scores, cluster_method = cluster_method)\n",
    "\n",
    "  # Compute symmetry\n",
    "  impact_scores_symmetric = {'weighted': impact_scores}\n",
    "  for cluster_method, adj_matrix in impact_scores_binary.items():\n",
    "    impact_scores_symmetric[cluster_method+'_directed'] = adj_matrix\n",
    "    impact_scores_symmetric[cluster_method+'_up'] = (adj_matrix + adj_matrix.T > 0.5).astype(float)\n",
    "    impact_scores_symmetric[cluster_method+'_down'] = (adj_matrix + adj_matrix.T > 1.5).astype(float)\n",
    "\n",
    "  return impact_scores_symmetric\n",
    "\n",
    "\n",
    "\n",
    "def reduce_to_argmax(snapshot_tensor):\n",
    "  snapshot_tensor = snapshot_tensor.squeeze()\n",
    "  if len(snapshot_tensor.shape) > 1:\n",
    "    #print(\"reduce snapshot\")\n",
    "    snapshot_tensor = torch.argmax(snapshot_tensor,dim=1)  #todo does not work because might be same\n",
    "  return snapshot_tensor.flatten()\n",
    "\n",
    "\n",
    "def convert_snapshots_to_TS(train_loader, test_loader):\n",
    "  print(\"each snapshot has dim:\", test_loader.dataset[0].shape)\n",
    "  node_num, _= get_node_num_and_dim_from_loader(train_loader)\n",
    "  observations = len(train_loader.dataset) + len(test_loader.dataset)\n",
    "  TS = np.zeros([node_num, observations])\n",
    "  for i in range(len(train_loader.dataset)):\n",
    "    TS[:,i] = reduce_to_argmax(train_loader.dataset[i])\n",
    "  shift = len(train_loader.dataset)\n",
    "  for i in range(len(test_loader.dataset)):\n",
    "    TS[:,i+shift] = reduce_to_argmax(test_loader.dataset[i])\n",
    "  return TS\n",
    "\n",
    "\n",
    "BASELINE_METHODS = {\n",
    "          'CorrelationMatrix': netrd.reconstruction.CorrelationMatrix(),\n",
    "          'MutualInformationMatrix': netrd.reconstruction.MutualInformationMatrix(),\n",
    "          'PartialCorrelationMatrix': netrd.reconstruction.PartialCorrelationMatrix()}\n",
    "\n",
    "def get_specific_baseline(TS, recon):\n",
    "  start_time = time.time()\n",
    "  _ = recon.fit(TS, threshold_type='degree', avg_k = 5)  #avg_k should not matter\n",
    "  W = recon.results['weights_matrix']  # todo symmetric?\n",
    "  time_elapsed = time.time()-start_time\n",
    "  return W, time_elapsed\n",
    "\n",
    "def get_baseline(train_loader, test_loader):\n",
    "  time_elapsed = dict()\n",
    "  TS = convert_snapshots_to_TS(train_loader, test_loader)\n",
    "  print(\"TS is\", TS.shape, TS )\n",
    "  weighted_adj_matrix_dict = dict()\n",
    "  for name, recon in BASELINE_METHODS.items():\n",
    "    weighted_adj_matrix, time = get_specific_baseline(TS, recon)\n",
    "    weighted_adj_matrix_dict[name] = weighted_adj_matrix\n",
    "    time_elapsed[name] = time\n",
    "\n",
    "  binary_adj_matrix_dict = dict()\n",
    "  time_elapsed_dict_new = dict()\n",
    "  for name, weighted_adj_matrix in weighted_adj_matrix_dict.items():\n",
    "    results = binarize_weight_matrix_multi(weighted_adj_matrix)\n",
    "    for binarization_name, binary_adj_matrix in results.items():\n",
    "      binary_adj_matrix_dict[name+'_'+binarization_name] = binary_adj_matrix\n",
    "      time_elapsed_dict_new[name+'_'+binarization_name] = time_elapsed[name]\n",
    "\n",
    "  return binary_adj_matrix_dict, time_elapsed_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582ca6f-9484-4fb4-ab48-3542c9d142dc",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f34c-8c35-4fe1-a123-44de5b11bd06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_cascade(G, agent_rng, steps=None, become_active_prob = 0.5):\n",
    "  record = list()\n",
    "  S = [1, 0, 0]\n",
    "  R = [0, 1, 0]\n",
    "  I = [0, 0, 1]\n",
    "\n",
    "\n",
    "  if steps is None:\n",
    "    steps = 5000\n",
    "  states = [S] * G.number_of_nodes()\n",
    "  states[random.choice(range(G.number_of_nodes()))] = I\n",
    "  become_active_prob_of_n = [agent_rng.random() for i in range(G.number_of_nodes())]\n",
    "\n",
    "\n",
    "  for _ in range(100000):\n",
    "    new_states = list(states)\n",
    "    for n in G.nodes():\n",
    "      if states[n] == I:\n",
    "        new_states[n] = R\n",
    "        continue\n",
    "      if states[n] == S and len([n_j for n_j in G.neighbors(n) if states[n_j] == I]) > 0:\n",
    "        if random.random() < become_active_prob_of_n[n]:\n",
    "            new_states[n] = I\n",
    "        else:\n",
    "            new_states[n] = R\n",
    "\n",
    "    states = list(new_states)\n",
    "    record.append(list(states))\n",
    "    if len([n_j for n_j in G.nodes() if states[n_j] == I]) == 0:\n",
    "        break\n",
    "\n",
    "  last_state = record[-1]\n",
    "  last_state = [s[0:2] for s in last_state]\n",
    "  return last_state \n",
    "\n",
    "\n",
    "def gen_mixedsis(G, agent_rng, inf_rate=1.0, rec_rate=2.0, noise=0.1):\n",
    "  S = [1., 0.]\n",
    "  I = [0., 1.]\n",
    "  steps = 1000 + random.choice(range(1000))\n",
    "  states = [random.choice([S, I]) for i in range(G.number_of_nodes())]\n",
    "  inf_rate_list = agent_rng.exponential(scale=inf_rate, size=G.number_of_nodes())\n",
    "  rec_rate_list = agent_rng.exponential(scale=rec_rate, size=G.number_of_nodes())\n",
    "  for _ in range(steps):\n",
    "    rates = np.zeros(G.number_of_nodes())\n",
    "    for n in range(G.number_of_nodes()):\n",
    "      rates[n] = noise\n",
    "      if states[n] == I:\n",
    "        rates[n] += rec_rate_list[n]\n",
    "      if states[n] == S:\n",
    "        rates[n] += np.sum([inf_rate_list[n_j] for n_j in G.neighbors(n) if states[n_j] == I])\n",
    "      rates[n] = 1.0/rates[n]\n",
    "    jump_time = np.random.exponential(rates)\n",
    "    change_n = np.argmin(jump_time)\n",
    "    states[change_n] = S if states[change_n] == I else I\n",
    "  return states\n",
    "\n",
    "\n",
    "# opinion\n",
    "def gen_voterpartinv(G, agent_rng, noise=0.01):\n",
    "  A = [1., 0.]\n",
    "  B = [0., 1.]\n",
    "  steps = 1000 + random.choice(range(1000))\n",
    "  states = [random.choice([A, B]) for i in range(G.number_of_nodes())]\n",
    "  type_of_node = [agent_rng.random() for i in range(G.number_of_nodes())] # <0.5 is voter, >=0.5 is inv voter\n",
    "  for _ in range(steps):\n",
    "    rates = np.zeros(G.number_of_nodes())\n",
    "    for n in range(G.number_of_nodes()):\n",
    "      if type_of_node[n] >= 0.5:\n",
    "        rates[n] = len([n_j for n_j in G.neighbors(n) if states[n_j] == states[n]]) + noise\n",
    "      else:\n",
    "        rates[n] = len([n_j for n_j in G.neighbors(n) if states[n_j] != states[n]]) + noise\n",
    "      rates[n] = 1.0/rates[n] \n",
    "    jump_time = np.random.exponential(rates)\n",
    "    change_n = np.argmin(jump_time)\n",
    "    states[change_n] = A if states[change_n] == B else B\n",
    "  return states\n",
    "\n",
    "\n",
    "def gen_majority(G, agent_rng, change_rate=1.0, noise=0.05):\n",
    "    A = [1., 0.]\n",
    "    B = [0., 1.]\n",
    "    steps = 1000 + random.choice(range(1000))\n",
    "    states = [random.choice([A, B]) for i in range(G.number_of_nodes())]\n",
    "    slow_down = [agent_rng.random() for i in range(G.number_of_nodes())]\n",
    "    for _ in range(steps):\n",
    "        rates = np.zeros(G.number_of_nodes())\n",
    "        for n in range(G.number_of_nodes()):\n",
    "            rates[n] = noise\n",
    "            neig_A = len([n_j for n_j in G.neighbors(n) if states[n_j] == A])\n",
    "            neig_B = len([n_j for n_j in G.neighbors(n) if states[n_j] == B])\n",
    "            if states[n] == A and neig_B > neig_A:\n",
    "                rates[n] += change_rate\n",
    "            if states[n] == B and neig_A > neig_B:\n",
    "                rates[n] += change_rate\n",
    "            rates[n] *= slow_down[n]\n",
    "            rates[n] = 1.0/rates[n] # numpy uses mean as rate param\n",
    "        jump_time = np.random.exponential(rates)\n",
    "        change_n = np.argmin(jump_time)\n",
    "        states[change_n] = A if states[change_n] == B else B\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c381d-8b4f-492c-9cbd-028289da53e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01093456-c846-4ba3-b3f3-95fcb50b230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snapshots(graph_nx, dynamics, num_samples, exp_name):\n",
    "\n",
    "  assert all(char.isalpha() for char in dynamics)\n",
    "  func = eval(\"gen_\"+dynamics)\n",
    "\n",
    "  snapshots = list()\n",
    "  print(\"create snapshots for\", dynamics)\n",
    "  for i in tqdm(range(num_samples)):\n",
    "    seed = str_to_int(exp_name)\n",
    "    if DIFFERENT_PARAMS_FOR_SNAPSHOTS:\n",
    "      seed += i\n",
    "    agent_rng = np.random.RandomState(seed)\n",
    "    snapshots.append(func(graph_nx, agent_rng))\n",
    "\n",
    "  plot_snapshots(graph_nx, snapshots, exp_name)\n",
    "\n",
    "  dataset = list()\n",
    "  for s in snapshots:\n",
    "    # check if dim == 1\n",
    "    if len(s[0]) == 2:  \n",
    "      s = torch.Tensor(s) * 2.0 - 1.0  # we want -1 values instaed of 0, 0 are for masked values\n",
    "      s = s[:,:-1].squeeze()  # the last column is redundant\n",
    "    else:\n",
    "      s = torch.Tensor(s)\n",
    "    dataset.append(s)\n",
    "  return dataset\n",
    "\n",
    "def get_dataloder(graph_nx, dynamics, exp_name):\n",
    "  train_set = create_snapshots(graph_nx, dynamics, get_config(\"num_train_sample\"), exp_name)\n",
    "  train_loader = DataLoader(train_set, batch_size=get_config(\"batch_size\"), shuffle=True)\n",
    "\n",
    "  test_set = create_snapshots(graph_nx, dynamics, get_config(\"num_test_sample\"), exp_name)\n",
    "  test_loader = DataLoader(test_set, batch_size=get_config(\"batch_size\"), shuffle=True)\n",
    "\n",
    "  return train_loader, test_loader\n",
    "\n",
    "\n",
    "def run_get_dataloder(exp_name, graph_nx, dynamics):\n",
    "  pickle_path = f\"{exp_name}_snapshots.pickle\"\n",
    "\n",
    "  if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "      train_loader, test_loader = pickle.load(f)\n",
    "      print(\"Found \", pickle_path)\n",
    "      return train_loader, test_loader\n",
    "  else:\n",
    "    print(pickle_path, \"not found\")\n",
    "    train_loader, test_loader = get_dataloder(graph_nx, dynamics, exp_name)\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "      pickle.dump((train_loader, test_loader), f)\n",
    "\n",
    "  return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead3a64-a47f-466b-97ec-079f2ce083ea",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106477de-f361-4b78-a041-620f20e2ce33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self, node_num=9, node_dim=1, num_layers=get_config(\"layer_num\"), clamp_output = True):\n",
    "    super(MLP, self).__init__()\n",
    "    print(f\"create model with node_num {node_num} and node dim {node_dim}\")\n",
    "    self.clamp_output = clamp_output\n",
    "    self.node_dim = node_dim\n",
    "    self.input_size = node_num * node_dim\n",
    "    self.hidden_dim = get_config(\"hidden_size_scale\") * self.input_size\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    self.layers.append(nn.Linear(self.input_size, self.hidden_dim))\n",
    "    for _ in range(num_layers - 2):\n",
    "      self.layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "    self.layers.append(nn.Linear(self.hidden_dim, node_dim))\n",
    "\n",
    "    self.dropout = nn.Dropout(get_config(\"dropout\"))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # fix dims\n",
    "    if len(x.shape) == 1:\n",
    "      x = x.view(1,-1,1)\n",
    "    if len(x.shape) == 2:\n",
    "      x = x.view(x.shape[0],x.shape[1],1)\n",
    "    assert(len(x.shape) == 3)\n",
    "    batch_dim = x.shape[0]\n",
    "    node_num = x.shape[1]\n",
    "    node_dim = x.shape[2]\n",
    "    x = x.view(-1, node_num * node_dim)\n",
    "    assert(x.shape[0] == batch_dim)\n",
    "\n",
    "    # actual forward pass\n",
    "    x = self.dropout(x)\n",
    "    for layer in self.layers[:-1]:\n",
    "      x = F.relu(layer(x))\n",
    "\n",
    "    x = self.layers[-1](x)\n",
    "    if self.clamp_output:\n",
    "      x = (torch.sigmoid(x) * 2.1) - 1.0 # values between -1 and 1, use 2.1 to give a little bit of extra space\n",
    "    else:\n",
    "      x = F.relu(x)\n",
    "    assert(batch_dim == x.shape[0] and node_dim == x.shape[1])\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the MLP\n",
    "model = MLP(node_num=20, node_dim=3)\n",
    "\n",
    "# Test it with random data\n",
    "dataset = torch.randn(10,20,3)\n",
    "output = model(dataset)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3003a-ec27-4193-a9ef-0215a9d5162c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(node_index, train_loader):\n",
    "  node_num, node_dim = get_node_num_and_dim_from_loader(train_loader)\n",
    "  model = MLP(node_num=node_num-1, node_dim=node_dim)\n",
    "  criterion = nn.MSELoss()  # Use appropriate loss function for your problem\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in tqdm(range(get_config(\"num_epoch\"))):  # 100 epochs, adjust as needed\n",
    "    epoch_loss = list()\n",
    "    for data in train_loader:\n",
    "      data_in, data_gt = exclude_index(data, node_index)#\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      output = model(data_in)\n",
    "\n",
    "      # Calculate the loss\n",
    "      loss = criterion(data_gt.squeeze(), output.squeeze())\n",
    "      epoch_loss.append(loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  print(f\"Epoch {epoch+1}, Loss: {np.mean(epoch_loss)}\")\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a80ae-eb89-4bf9-8050-b04a7d1b3ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reorder_rows(data):\n",
    "  #print(\"there should be a one in each row\", data.shape, data)\n",
    "  assert len(data.shape) == 2\n",
    "  rand_columns = [0,2,1]\n",
    "  random.shuffle(rand_columns)\n",
    "  while rand_columns == [0,1,2]:\n",
    "    random.shuffle(rand_columns)\n",
    "  data_new = torch.index_select(data, 1, torch.LongTensor(rand_columns))\n",
    "  return data_new\n",
    "\n",
    "\n",
    "def test_model_saliency(model, node_index, test_loader):\n",
    "  node_num, _= get_node_num_and_dim_from_loader(test_loader)\n",
    "\n",
    "  impact_score = torch.zeros(node_num-1)\n",
    "  model.train()\n",
    "\n",
    "  for data in test_loader:\n",
    "    model.zero_grad()\n",
    "    data_in, data_gt = exclude_index(data, node_index)\n",
    "    data_in = data_in.view(data_in.shape[0], data_in.shape[1], -1)\n",
    "    data_in.requires_grad = True\n",
    "    output = model(data_in)\n",
    "    output = torch.sum(torch.abs(output))\n",
    "    output.backward()\n",
    "    importance_score = torch.sum(torch.abs(data_in.grad),dim=(0,2))\n",
    "    print(\"importance_score\", importance_score.shape, importance_score)\n",
    "    impact_score = impact_score + importance_score\n",
    "\n",
    "  impact_score = impact_score.detach().cpu().flatten().numpy()\n",
    "  return impact_score\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, node_index, test_loader, method):\n",
    "  assert(method in [\"permutation\", \"masking\", \"saliency\"])\n",
    "  if method == \"saliency\":\n",
    "    return test_model_saliency(model, node_index, test_loader)\n",
    "  node_num, _= get_node_num_and_dim_from_loader(test_loader)\n",
    "\n",
    "  impact_score = torch.zeros(node_num-1)\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  model.eval()\n",
    "\n",
    "  for data in test_loader:\n",
    "    #print(\"shape of data: \",data.shape, data)\n",
    "    data_in, data_gt = exclude_index(data, node_index)\n",
    "    output = model(data_in)\n",
    "    loss_baseline = criterion(data_gt.squeeze(), output.squeeze())\n",
    "    for alterd_i in range(node_num-1):\n",
    "      data_in_altered = data_in.clone()\n",
    "      if method == \"masking\":\n",
    "        data_in_altered[:, alterd_i] =  0.0\n",
    "      elif method == \"permutation\":\n",
    "        if model.node_dim == 1:\n",
    "          data_in_altered[:, alterd_i] =  -1.0 * data_in_altered[:, alterd_i] #resample\n",
    "        else:\n",
    "          data_in_altered[:, alterd_i, :] =  reorder_rows(data_in_altered[:, alterd_i, :].squeeze()) #this is still wrong\n",
    "      else:\n",
    "        raise ValueError(\"method not found\")\n",
    "      assert(data_in_altered.shape == data_in.shape)\n",
    "      output_altered = model(data_in_altered)\n",
    "      loss_i = criterion(data_gt.squeeze(), output_altered.squeeze())\n",
    "      loss_relative_change = loss_i/(loss_baseline+0.00000000001)\n",
    "      impact_score[alterd_i] += loss_relative_change\n",
    "  impact_score = impact_score.detach().cpu().flatten().numpy()\n",
    "  return impact_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72baaf7f-42ac-4bb7-858c-0559026eb562",
   "metadata": {},
   "source": [
    "## Graph Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a63e5-38e5-4ad3-81fc-aade0f050557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_graph(train_loader, test_loader, method=False):\n",
    "  assert(method in [\"permutation\", \"masking\", \"saliency\"])\n",
    "  node_num, _= get_node_num_and_dim_from_loader(train_loader)\n",
    "  weighted_adj_matrix = np.zeros([node_num, node_num])\n",
    "  time_start = time.time()\n",
    "  if get_config(\"use_train_as_test\"):\n",
    "    test_loader = train_loader # todo, concat datasets?\n",
    "\n",
    "  for v_i in range(node_num):\n",
    "    model = train_model(v_i, train_loader)\n",
    "    impact_score = test_model(model, v_i, test_loader, method=method)\n",
    "    impact_score = impact_score/np.sum(impact_score)\n",
    "    impact_score = impact_score.flatten()\n",
    "    weighted_adj_matrix[v_i, :v_i] = impact_score[:v_i]\n",
    "    weighted_adj_matrix[v_i, v_i+1:] = impact_score[v_i:]\n",
    "\n",
    "  time_elapsed = time.time() - time_start\n",
    "\n",
    "  binary_adj_matrix_dict = binarize_weight_matrix_multi(weighted_adj_matrix)\n",
    "  binary_adj_matrix_dict = {f'our method ({k})': v for k,v in binary_adj_matrix_dict.items()}\n",
    "  time_elapsed_dict = {name: time_elapsed for name in binary_adj_matrix_dict.keys()}\n",
    "\n",
    "  return binary_adj_matrix_dict, time_elapsed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec071f8a-2349-4574-ba22-bb24453e0fa1",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309de591-c0ac-42cb-8fcd-2ffca40571ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_graph_distance_dict(adj_dict, adj_matrix_gt):\n",
    "  results = dict()\n",
    "  for name, adj_matrix_pred in adj_dict.items():\n",
    "    results[name] = compute_graph_distance(adj_matrix_pred, adj_matrix_gt)\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c640616-0236-4a4d-9d0b-bacc0d279864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experiment(exp_name, graph, dynamics):\n",
    "  set_seed(f\"{exp_name}_snapshot\")\n",
    "  adj_matrix_gt = nx.to_numpy_array(graph)\n",
    "\n",
    "  # our method\n",
    "  train_loader, test_loader =  run_get_dataloder(exp_name, graph, dynamics)\n",
    "  set_seed(exp_name + \"_infergraph\")\n",
    "\n",
    "  # with permutation\n",
    "  binary_adj_dict, time_elapsed_dict = infer_graph(train_loader, test_loader, method=\"permutation\")\n",
    "  graph_distance_dict = get_graph_distance_dict(binary_adj_dict, adj_matrix_gt)\n",
    "  print(\"graph_distance_dict: \", graph_distance_dict)\n",
    "\n",
    "  # with masking\n",
    "  binary_adj_mask_dict, time_elapsed_mask_dict = infer_graph(train_loader, test_loader, method=\"masking\")\n",
    "  graph_distance_mask_dict = get_graph_distance_dict(binary_adj_mask_dict, adj_matrix_gt)\n",
    "  print(\"graph_distance_dict: \", graph_distance_mask_dict)\n",
    "  for name, dist in graph_distance_mask_dict.items():\n",
    "    graph_distance_dict[name + \" (masked)\"] = dist\n",
    "\n",
    "  # with saliency\n",
    "  binary_adj_mask_dict, time_elapsed_mask_dict = infer_graph(train_loader, test_loader, method=\"saliency\")\n",
    "  graph_distance_mask_dict = get_graph_distance_dict(binary_adj_mask_dict, adj_matrix_gt)\n",
    "  print(\"graph_distance_dict: \", graph_distance_mask_dict)\n",
    "  for name, dist in graph_distance_mask_dict.items():\n",
    "    graph_distance_dict[name + \" (saliency)\"] = dist\n",
    "\n",
    "\n",
    "  # visualize our method\n",
    "  for name, adj_matrix in binary_adj_dict.items():\n",
    "    print(name)\n",
    "    visualize_matrix(adj_matrix, exp_name+f\"_impact_scores_{name}.png\")\n",
    "  visualize_matrix(adj_matrix_gt, exp_name+\"_adj_matrix_gt.png\")\n",
    "\n",
    "  # baseline\n",
    "  set_seed(exp_name + \"_baseline\")\n",
    "  binary_adj_dict_baseline, time_elapsed_dict_baseline = get_baseline(train_loader, test_loader)\n",
    "  graph_distance_dict_baseline = get_graph_distance_dict(binary_adj_dict_baseline, adj_matrix_gt)\n",
    "\n",
    "  # merge\n",
    "  graph_distance_dict.update(graph_distance_dict_baseline)\n",
    "  time_elapsed_dict.update(time_elapsed_dict_baseline)\n",
    "\n",
    "\n",
    "  results = {'adj_matrix_gt': adj_matrix_gt, \"adj_matrix_pred\": binary_adj_dict, \"adj_matrix_baseline\": graph_distance_dict_baseline}\n",
    "\n",
    "  return graph_distance_dict, time_elapsed_dict, results\n",
    "\n",
    "\n",
    "def run_experiment(exp_name, graph, dynamics):\n",
    "  pickle_path = exp_name + '_solution.pickle'\n",
    "\n",
    "  if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "      graph_distance_dict, time_elapsed_dict, results = pickle.load(f)\n",
    "      print(\"Found \", pickle_path)\n",
    "      print(\"Graph distance: \", graph_distance_dict)\n",
    "  else:\n",
    "    graph_distance_dict, time_elapsed_dict, results = experiment(exp_name, graph, dynamics)\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "      pickle.dump((graph_distance_dict, time_elapsed_dict, results), f)\n",
    "\n",
    "  return graph_distance_dict, time_elapsed_dict, results\n",
    "\n",
    "def save_src_file():\n",
    "  for python_file in sorted(glob.glob('*.ipynb')):\n",
    "    wandb.log_artifact(python_file, name=f\"src_ipynb_{SWEEP_ID}\", type=\"my_dataset\")\n",
    "  for python_file in sorted(glob.glob('*.py')):\n",
    "    wandb.log_artifact(python_file, name=f\"src_py_{SWEEP_ID}\", type=\"my_dataset\")\n",
    "\n",
    "    \n",
    "def extract_positions(graph_distance_dict):\n",
    "  print(\"extract pos\")\n",
    "  print(\"graph dist dict \", graph_distance_dict)\n",
    "  graph_distance_list = list(graph_distance_dict.items())\n",
    "  gd_list = sorted(graph_distance_list, key = lambda x: x[1])\n",
    "  gd_list = [method for method, _ in gd_list]\n",
    "  print(\"gd_list \", gd_list)\n",
    "  pos_dict = {method: gd_list.index(method) for method in graph_distance_dict}\n",
    "  print(\"pos_dict \", pos_dict)\n",
    "  return pos_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fe65c-f987-4a2c-be6f-b0a2c8c39068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_agent():\n",
    "  results = {\"exp_name\": list(), \"method\": list(), \"runtime\": list(), \"graph_distance\": list(), \"position\": list()}\n",
    "  graphs = get_graph_generators()\n",
    "  graph_names = sorted(graphs.keys())\n",
    "  prefix = str(PROJECT_NAME + str(int(random.random()*10000))) #+'_'+str(SWEEP_ID)+'_'+str(int(random.random()*10000))\n",
    "  print(\"running\", prefix)\n",
    "\n",
    "  dynamical_models = sorted([var.replace(\"gen_\",\"\") for var in globals() if var.startswith(\"gen_\")])\n",
    "  dynamical_models = ['cascade', 'majority', 'mixedsis', 'voterpartinv']\n",
    "  position_store = list()\n",
    "\n",
    "\n",
    "  # actual runs\n",
    "  counter = 0\n",
    "  for run_i in range(NUM_RUNS):\n",
    "    for graphname in graph_names:\n",
    "      graph = graphs[graphname]\n",
    "      for dynamics in dynamical_models:\n",
    "        counter += 1\n",
    "        exp_name = f\"exp_{prefix}_{graphname}_{dynamics}_{run_i+1:03}\" # \"exp_\"+graphname+\"_\"+dynamics+\"_\"+str(i+1).zfill(3)\n",
    "        print(\"start experiment:\", exp_name)\n",
    "        graph_distance_dict, time_elapsed_dict, _ = run_experiment(exp_name, graph, dynamics)\n",
    "        graph_position_dict = extract_positions(graph_distance_dict)\n",
    "        results = add_exp_to_results(exp_name, results, graph_distance_dict, graph_position_dict, time_elapsed_dict, filename=f\"results_{prefix}.csv\")\n",
    "        p = get_position(graph_distance_dict)\n",
    "        position_store.append(p)\n",
    "        print(\"position\",p)\n",
    "        if USE_WANDB:\n",
    "          wandb.log({\"position\": p, \"mean position\": np.mean(position_store), \"counter\": counter})\n",
    "          wandb.log_artifact(f\"results_{prefix}.csv\", name=f\"results_{prefix}.csv\", type=\"my_dataset\")\n",
    "          wandb.log({\"results\": wandb.Table(dataframe=pd.read_csv(f\"results_{prefix}.csv\"))})\n",
    "\n",
    "        # final logging (each iteration)\n",
    "        df = pd.DataFrame(results)\n",
    "        df = df[df['method'].str.contains('hierachical_down')]\n",
    "        df = df.sort_values(by=['exp_name', 'graph_distance'])\n",
    "        df.to_csv(f\"results_sorted_{prefix}.csv\")\n",
    "        if USE_WANDB:\n",
    "          wandb.log_artifact(f\"results_sorted_{prefix}.csv\", name=f\"results_sorted_{prefix}.csv\", type=\"my_dataset\")\n",
    "          wandb.log_artifact(f\"results_{prefix}.csv\", name=f\"results_{prefix}.csv\", type=\"my_dataset\")\n",
    "          wandb.log({\"table_sorted\": wandb.Table(dataframe=df)})\n",
    "\n",
    "\n",
    "  # final positions (only once)\n",
    "  df = pd.DataFrame(results)\n",
    "  mean_values = df.groupby('method')['position'].mean()\n",
    "  result_list = list(mean_values.items())\n",
    "    \n",
    "  # sort by method name\n",
    "  result_list = sorted(result_list, key = lambda x: x[0] if \"our method\" not in x[0] else ' '+x[0]) # our method should be first\n",
    "  print(\"final absolute positions\\n\", result_list, \"\\n and now line by line:\")\n",
    "  for k,v in result_list:\n",
    "    print(k, v)\n",
    "  result_list_value = [value for method, value in result_list]\n",
    "  if USE_WANDB:\n",
    "    for i, value in enumerate(result_list_value):\n",
    "      wandb.log({\"absolute positions\": value, \"counter_ap\":i})\n",
    "    \n",
    "  # sort by mean position\n",
    "  result_list = sorted(result_list, key = lambda x: x[1])\n",
    "  print(\"methods sorted\")\n",
    "  for method, mean_pos in result_list:\n",
    "    print(method, mean_pos)\n",
    "  df = pd.DataFrame(result_list, columns=['Method', 'Mean Pos'])\n",
    "  df.to_csv(f\"results_meanpos_{prefix}.csv\", index=False)\n",
    "  if USE_WANDB:\n",
    "    wandb.log_artifact(f\"results_meanpos_{prefix}.csv\", name=f\"results_meanpos_{prefix}.csv\", type=\"my_dataset\")\n",
    "    wandb.log({\"table_mean_pos\": wandb.Table(dataframe=df)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "  with wandb.init():\n",
    "    save_src_file()\n",
    "    try:\n",
    "      return start_agent()\n",
    "    except Exception as e:\n",
    "      error_message = traceback.format_exc()\n",
    "      print(\"final in main error:\\n\", error_message)\n",
    "      with open('_error_log.txt', 'a') as f:\n",
    "        f.write(error_message + '\\n')\n",
    "      time.sleep(1)\n",
    "\n",
    "def start_with_wandb():\n",
    "  global SWEEP_ID, USE_WANDB\n",
    "  print(\"start experiments\")\n",
    "  USE_WANDB = True\n",
    "  os.environ[\"WANDB_MODE\"] = WANDB_MODE\n",
    "  try:\n",
    "    SWEEP_ID = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
    "    wandb.agent(SWEEP_ID, function=main, count=1)\n",
    "  except Exception as e:\n",
    "    error_message = traceback.format_exc()\n",
    "    print(\"final error:\\n\", error_message)\n",
    "    with open('_error_log.txt', 'a') as f:\n",
    "      f.write(error_message + '\\n')\n",
    "    time.sleep(10)\n",
    "\n",
    "def start_without_wandb():\n",
    "  global SWEEP_ID, USE_WANDB\n",
    "  USE_WANDB = False\n",
    "  SWEEP_ID = \"00000000\"\n",
    "  start_agent()\n",
    "\n",
    "set_seed('42')\n",
    "for _ in range(20):\n",
    "  start_with_wandb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
